---
title: 'Chiron - Part 6: Chat'
pubDate: 2024-11-23 10:15:00
description: "An ongoing series covering the development of a personal AI assistant"
tags: ["LLM","AI"]
image: "https://raw.githubusercontent.com/nduartech/nduartech.github.io/master/packages/host/src/assets/chironChat.jpeg"
---
import {Image} from "astro:assets";
import chironChat from "../../assets/chironChat.jpeg";


<Image
    src={chironChat}
    alt="Conversing"
    class="w-1/4 h-1/4 mt-7 mb-7 rounded-l-3xl float-end ml-7"
></Image>
<div class="reset-tw">
    After sorting out text-to-speech and speech-to-text functionality, the next crucial piece was implementing the core chat capability. This journey involved evaluating different language models and backend solutions, ultimately leading to some interesting engineering decisions.

-----
##### Model Selection

I began by conducting a comparative analysis of various models, focusing primarily on Llama 3.2 3B and Qwen 2.5 7B. The Llama model was tested at higher quantization levels, including fp16, while Qwen was evaluated at lower quantization levels. While I also experimented with alternatives like Phi, Neural-Chat, and various fine-tunes of both Llama 3.1 and Qwen 2.5, the standout performers were consistently Qwen 2.5, Llama 3.1, and 3.2. After extensive testing, I opted for Qwen 2.5 7B, as it provided the best balance of performance and resource utilization for my use case.

-----
##### Backend Evolution

The search for the right backend solution took several interesting turns:

**[Ollama](https://ollama.com/)**
Ollama is a lightweight model serving solution that aims to make running LLMs as simple as possible. It handles model management, provides a consistent API interface, and includes built-in model repository functionality. While appealing for its ease of use and Docker-like approach to model management, it presented two significant drawbacks:

- Integration would have increased binary size substantially due to its standalone nature
- Performance benchmarks showed it lagging behind llama.cpp alternatives
- Additionally, while Ollama's containerized approach is excellent for development and testing, it didn't align well with my goal of a lean, integrated solution



**[llama.cpp](https://github.com/ggerganov/llama.cpp)**
llama.cpp is a highly optimized C++ implementation for running LLaMA models, known for its exceptional performance and minimal resource footprint. It's particularly notable for its:

- Advanced quantization techniques for running large models on consumer hardware
- Extensive SIMD optimizations across multiple architectures
- Support for various acceleration backends (CUDA, OpenCL, Vulkan)

My implementation required some creative engineering:

- Implemented automated git repo checkout and build process
- Created shell scripts to handle server initialization
- Set up automatic GGUF model file downloads

However, llama.cpp's lack of tool use support (at the time) forced me to look elsewhere.



**[KoboldCPP](https://github.com/LostRuins/koboldcpp)**
KoboldCPP is a fascinating project that bridges the gap between llama.cpp's performance and more feature-rich inference servers. It's essentially a Python wrapper around llama.cpp that adds:

- OpenAI-compatible API endpoints
- Built-in support for function calling/tool use
- Streaming capabilities for both regular chat and completion endpoints
- Advanced text generation settings like typical sampling and repetition penalties

It offered the workable compromise I needed:

- Supported streaming chat functionality
- Provided tool use capability (though non-streaming)
- Maintained OpenAI-compatible endpoints with minimal variations
- Retained most of llama.cpp's performance benefits

The end result is a responsive chat system that maintains fluid interaction while properly coordinating between text display and voice output, all while running entirely locally on consumer hardware.
</div>